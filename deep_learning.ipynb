{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Behavior Prediction using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Tatsiana Mihai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project I'll use public datasets with user behavior information available on Kaggle:\n",
    "- Model training: E-commerce behavior data from multi category store for November 2019\n",
    "https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store\n",
    "- Validating model robustness: E-commerce behavior data from multi category store for January 2020 https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: due to Github size constraints `data` folder is not provided in this repository. You can download source files by using links mentioned in the Project Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read csv file and get the number of rows\n",
    "\n",
    "# training dataset\n",
    "# df = pd.read_csv('data/2019-Nov.csv')\n",
    "\n",
    "# testing dataset\n",
    "df = pd.read_csv('data/2020-Jan.csv')\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first rows in the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the quality of the data. Firts, I want to make sure there're no rows with null or invalid values that might affect model training. \n",
    "Let's check which columns have null values and how many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sum of null values for each column\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there're quite many missed values for `category_code` and `brand`, and also 10 missed values for the `user_session`. \n",
    "Now let's check if there're negative `price` values for as they're invalid and will affect the accuracy of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sum of negative values for the column `price`\n",
    "\n",
    "(df.price < 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all prices are greater than `0` which makes the dataset is pretty clean. The only issue I'd like to address is the missed values. Also, I'd like to transform composite values in the `category_column` into multi-dimentional features to later try different combinations for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reduce dimensionality**  \n",
    "The dataset is large, however not all of the columns are signficant for model training. The `user_session` column can be dropped as the `user_id` colimn contains all necessary information without missed values. Therefore, I'll remove `user_id` from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only necessary columns\n",
    "required_cols = [\n",
    "    'event_time',\n",
    "    'event_type',\n",
    "    'category_code',\n",
    "    'product_id',\n",
    "    'category_id',\n",
    "    'brand',\n",
    "    'price',\n",
    "    'user_id'\n",
    "]\n",
    "\n",
    "df = df[required_cols]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling the missing values**\n",
    "\n",
    "As we can see from the data exploration, there're 9224078 missed values in the `brand` column. As there's still other information such as `event_type`, `user_id` and `price`, which can be useful for ML training, I'll fill it with the value `unknown`. \n",
    "Apart fron that, some of `category_code` values contain `NaN` instead of expected string values which causes issues with data transformation. I'll fill them with `unknown` values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['brand'] = df['brand'].fillna('unknown')\n",
    "df['category_code'] = df['category_code'].fillna('unknown')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many categories we can restore by mapping their known brands. To do that, I'll get a list of unique combinations of unknown category codes and known brands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract uniquecvalues from `category_code` and `brand` columns\n",
    "\n",
    "unknown_cats = df[(df.category_code == 'unknown') & (df.brand != 'unknown')]\n",
    "unknown_cats = pd.unique(unknown_cats['brand'])\n",
    "\n",
    "len(unknown_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explore if it's possible to re-use category that defined for another product with the same `brand` value. To do that I'll fetch all categories from `category_code` for the brands listed in the `unknown_cats`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to find categories for the brands\n",
    "\n",
    "known_brands = df[(df.category_code != 'unknown') & (df.brand.isin(unknown_cats))]\n",
    "known_brands = known_brands[['category_code', 'brand']]\n",
    "brands_possible_cats = known_brands.groupby(by=['brand']).nunique().reset_index()\n",
    "\n",
    "# group brands to see how many potential categories each brand has\n",
    "grouped_known_brands = brands_possible_cats.groupby(by=['category_code']).count()\n",
    "grouped_known_brands.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the plot shows, a big chunk of brands has just one possible category. Let's explore some of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands_with_single_cat = brands_possible_cats[\\\n",
    "        brands_possible_cats.category_code == 1].brand\n",
    "\n",
    "known_brands[known_brands.brand.isin(brands_with_single_cat)][210:220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brands_with_multi_cat = brands_possible_cats[\\\n",
    "        brands_possible_cats.category_code > 25].brand\n",
    "\n",
    "known_brands[known_brands.brand.isin(brands_with_multi_cat)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "known_brands[known_brands.brand == 'xiaomi'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this moment it's obviout that only the first level category makes sence to be copied and added to the rows with missed values. If I add second or third category level I might affect the quality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data transformation**  \n",
    "First, I want to transform values from `category_code` into multi-column data `cat_1`, `cat_2`, etc.\n",
    "To make it I need to know the length of the longest chain in the `category_code` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the max number of categories\n",
    "\n",
    "max(df.category_code.transform(lambda x: x.str.split('.').transform(lambda y: len(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imputation**\n",
    "The maximum length of nested categories is `4`. Now it's possible to create new columns to store each category separately. As only the first layer of categories is to be filled for unknown categories, the other empty values can be filled with `unknown`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split `category_code` column into new columns\n",
    "df[['cat_1', 'cat_2', 'cat_3', 'cat_4']] = df.category_code.str.split(\".\", expand = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the values from the `category_code` column are transferred to another columns, it can be finally removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only necessary columns\n",
    "required_cols = [\n",
    "    'event_time',\n",
    "    'event_type',\n",
    "    'product_id',\n",
    "    'category_id',\n",
    "    'cat_1',\n",
    "    'cat_2',\n",
    "    'cat_3',\n",
    "    'cat_4',\n",
    "    'brand',\n",
    "    'price',\n",
    "    'user_id'\n",
    "]\n",
    "\n",
    "df = df[required_cols]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many new null varues have been created after changing the dimension of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sum of null values for each column\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cat_1` doesn't have empty values as it's been pre-filled with `unknown`. For the others the % of filling is 99.9% for the `cat_2`, 72.1% for the `cat_3`, and 32.4% for the `cat_4`. Though the last column doesn't look promising I'll keep it for know to be able to use it in future training and see how it affects the model.\n",
    "Let's fill the `unknown` values in `cat_1` with the values from known brands. To make it easier I change `known_brands` in the same way I changed the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split `category_code` column into new columns\n",
    "known_brands[['cat_1', 'cat_2', 'cat_3', 'cat_4']] = known_brands.category_code.str.split(\".\", expand = True)\n",
    "known_brands = known_brands[['brand', 'cat_1']].drop_duplicates(subset=['brand'])\n",
    "known_brands.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.merge(df, known_brands, on='brand', how='left')\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['cat'] = np.where(res['cat_1_x'] == 'unknown', res.cat_1_y, res.cat_1_x)\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keep only necessary columns\n",
    "required_cols = [\n",
    "    'event_time',\n",
    "    'event_type',\n",
    "    'product_id',\n",
    "    'category_id',\n",
    "    'cat',\n",
    "    'cat_2',\n",
    "    'cat_3',\n",
    "    'cat_4',\n",
    "    'brand',\n",
    "    'price',\n",
    "    'user_id'\n",
    "]\n",
    "\n",
    "res = res[required_cols]\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res['cat'] = res['cat'].fillna('unknown')\n",
    "res['cat_2'] = res['cat_2'].fillna('unknown')\n",
    "res['cat_3'] = res['cat_3'].fillna('unknown')\n",
    "res['cat_4'] = res['cat_4'].fillna('unknown')\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML specific preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data that is used for training model must be numeric. However, the dataset contains a few columns `string` or `datetime` types. First, let's convert `datetime` in the `event_time` into a `timetuple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "res.event_time = res.event_time.apply(lambda x: time.mktime(datetime.datetime.strptime(x,\n",
    "                                             \"%Y-%m-%d %H:%M:%S %Z\").timetuple()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other columns can be converted to numeric values by using label encoding. I'll use the `LabelEncoder` class that Scikit-learn provides for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the data\n",
    "categorical_columns = [\n",
    "    'event_type', \n",
    "    'cat', \n",
    "    'cat_2', \n",
    "    'cat_3', \n",
    "    'cat_4', \n",
    "    'brand'\n",
    "]\n",
    "\n",
    "for col in categorical_columns:\n",
    "    res[col] = label_encoder.fit_transform(res[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize price\n",
    "res['price'] = (res['price'] - res['price'].mean()) / res['price'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by 'event_time' \n",
    "res.sort_values(by='event_time', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe is ready for using in model training. I'll save it to a new .csv file to use it later for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving in a file\n",
    "# training dataset\n",
    "res.to_csv('data/processed_data_train.csv')\n",
    "\n",
    "# testinf dataset\n",
    "# res.to_csv('data/processed_data_test.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I repeat the same oreration for the testing dataset and save it to `data/processed_data_test.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_data = pd.read_csv('data/processed_data_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_generator(X, y, sequence_length, batch_size):\n",
    "    while True:\n",
    "        for i in range(0, len(X) - sequence_length, batch_size):\n",
    "            X_batch = [X[i+j:i+j+sequence_length] for j in range(batch_size)]\n",
    "            y_batch = y[i:i+batch_size]  # Adjust the slicing for y\n",
    "\n",
    "            # Pad sequences to ensure they have the same length\n",
    "            max_length = max(len(seq) for seq in X_batch)\n",
    "            X_batch = [np.pad(seq, ((0, max_length - len(seq)), (0, 0)), 'constant') for seq in X_batch]\n",
    "\n",
    "            X_batch = np.array(X_batch)\n",
    "            y_batch = np.array(y_batch)\n",
    "\n",
    "            yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = train_data[0:256000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "# Combine all features\n",
    "X = td[['event_time','product_id', 'category_id', 'cat', 'cat_2', 'cat_3', 'cat_4', 'brand', 'price']]\n",
    "X = scaler.fit_transform(X)\n",
    "y = td['event_type']\n",
    "y = to_categorical(y, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences from the data\n",
    "sequence_length = 100 \n",
    "batch_size = 256\n",
    "train_data_generator = sequence_generator(X_train, y_train, sequence_length, batch_size)\n",
    "test_data_generator = sequence_generator(X_test, y_test, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "input_layer = Input(shape=(sequence_length, X_train.shape[1]))\n",
    "x = Conv1D(64, kernel_size=3, activation='relu')(input_layer)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output_layer = Dense(3, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "cnn_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.1726 - accuracy: 0.9678 - val_loss: 0.1676 - val_accuracy: 0.9676\n",
      "Epoch 2/10\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.1611 - accuracy: 0.9681 - val_loss: 0.1663 - val_accuracy: 0.9676\n",
      "Epoch 3/10\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.1581 - accuracy: 0.9681 - val_loss: 0.1657 - val_accuracy: 0.9676\n",
      "Epoch 4/10\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.1558 - accuracy: 0.9681 - val_loss: 0.1659 - val_accuracy: 0.9676\n",
      "Epoch 5/10\n",
      "800/800 [==============================] - 8s 11ms/step - loss: 0.1536 - accuracy: 0.9681 - val_loss: 0.1675 - val_accuracy: 0.9676\n",
      "Epoch 6/10\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.1513 - accuracy: 0.9681 - val_loss: 0.1690 - val_accuracy: 0.9676\n",
      "Epoch 7/10\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.1487 - accuracy: 0.9681 - val_loss: 0.1703 - val_accuracy: 0.9676\n",
      "Epoch 8/10\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.1467 - accuracy: 0.9682 - val_loss: 0.1734 - val_accuracy: 0.9676\n",
      "Epoch 9/10\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.1431 - accuracy: 0.9683 - val_loss: 0.1733 - val_accuracy: 0.9676\n",
      "Epoch 10/10\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.1406 - accuracy: 0.9683 - val_loss: 0.1754 - val_accuracy: 0.9675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2856d74f0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using the generator\n",
    "cnn_model.fit(train_data_generator, \n",
    "          epochs=10, \n",
    "          steps_per_epoch=len(X_train) // batch_size,\n",
    "          validation_data=test_data_generator,          \n",
    "          validation_steps=len(X_test) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 5ms/step\n",
      "F1 Score: 0.951643440189553\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = cnn_model.predict(test_data_generator, steps=len(X_test) // batch_size)\n",
    "y_pred = np.argmax(y_pred, axis=1)  # Convert one-hot encoded predictions to class labels\n",
    "\n",
    "# Convert one-hot encoded true labels to class labels\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Second iteration - multiple improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences from the data\n",
    "sequence_length = 100 \n",
    "batch_size = 64\n",
    "train_data_generator = sequence_generator(X_train, y_train, sequence_length, batch_size)\n",
    "test_data_generator = sequence_generator(X_test, y_test, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "\n",
    "input_layer = Input(shape=(sequence_length, X_train.shape[1]))\n",
    "x = Conv1D(128, kernel_size=3, activation='relu')(input_layer)\n",
    "x = MaxPooling1D(pool_size=3)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_layer = Dense(3, activation='sigmoid')(x)  \n",
    "\n",
    "# Define a learning rate schedule\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps=100, decay_rate=0.9)\n",
    "\n",
    "# Use the learning rate schedule in the optimizer\n",
    "optimizer = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Compile the model\n",
    "cnn_imp_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "cnn_imp_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3200/3200 [==============================] - 20s 6ms/step - loss: 0.1838 - accuracy: 0.9678 - val_loss: 0.1645 - val_accuracy: 0.9677\n",
      "Epoch 2/30\n",
      "3200/3200 [==============================] - 20s 6ms/step - loss: 0.1735 - accuracy: 0.9681 - val_loss: 0.1646 - val_accuracy: 0.9677\n",
      "Epoch 3/30\n",
      "3200/3200 [==============================] - 20s 6ms/step - loss: 0.1734 - accuracy: 0.9681 - val_loss: 0.1649 - val_accuracy: 0.9676\n",
      "Epoch 4/30\n",
      "3200/3200 [==============================] - 21s 7ms/step - loss: 0.1728 - accuracy: 0.9681 - val_loss: 0.1647 - val_accuracy: 0.9677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x29a4bab00>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using the generator\n",
    "cnn_imp_model.fit(train_data_generator, \n",
    "          epochs=30, \n",
    "          steps_per_epoch=len(X_train) // batch_size,\n",
    "          validation_data=test_data_generator,          \n",
    "          validation_steps=len(X_test) // batch_size,\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 2s 2ms/step\n",
      "F1 Score: 0.9516922575117627\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = cnn_imp_model.predict(test_data_generator, steps=len(X_test) // batch_size)\n",
    "y_pred = np.argmax(y_pred, axis=1)  # Convert one-hot encoded predictions to class labels\n",
    "\n",
    "# Convert one-hot encoded true labels to class labels\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_data = pd.read_csv('data/processed_data_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "# Combine all features\n",
    "X = td[['event_time','product_id', 'category_id', 'cat', 'cat_2', 'cat_3', 'cat_4', 'brand', 'price']]\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "y = td['event_type']\n",
    "y = to_categorical(y, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "rnn_model = tf.keras.Sequential([\n",
    "    LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
    "    LSTM(units=64),\n",
    "    Dense(units=3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the input data to fit the model input shape\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Compile the model with accuracy as a metric\n",
    "rnn_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Train the model, including accuracy monitoring\n",
    "rnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess testing data and reshape it\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = rnn_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnn_model.predict(X_test)\n",
    "\n",
    "# Convert the continuous predictions to class labels\n",
    "y_test_int = np.argmax(y_test, axis=1)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test_int, y_pred_classes, average='micro')\n",
    "\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Second iteration - multiple improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               patience=3,          \n",
    "                               restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = tf.keras.Sequential([\n",
    "    LSTM(units=128, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
    "    LSTM(units=128, return_sequences=True),\n",
    "    Dropout(0.2),  \n",
    "    LSTM(units=128),\n",
    "    Dropout(0.2), \n",
    "    Dense(units=3, activation='softmax') \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.001)\n",
    "rnn_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.fit(X_train, y_train, \n",
    "              epochs=30,\n",
    "              validation_split=0.2,\n",
    "              callbacks=[early_stopping]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess testing data and reshape it\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = rnn_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have predictions from your model\n",
    "y_pred = rnn_model.predict(X_test)\n",
    "\n",
    "# Convert the continuous predictions to class labels (use argmax)\n",
    "y_test_int = np.argmax(y_test, axis=1)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test_int, y_pred_classes, average='micro')\n",
    "\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline comparison\n",
    "\n",
    "For the baseline I decided to use the Decision Tree machine learning algorithm. It's simpler than deep learning techniques but excels in sequential data prediction tasks because of the ability to recursively split data and follow tree-based structure to make a decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import packages required for decision tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the preprocessed datasets \n",
    "train_data = pd.read_csv('data/processed_data_train.csv')\n",
    "test_data = pd.read_csv('data/processed_data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features and the target\n",
    "Train_X = train_data.drop('event_type', axis=1)\n",
    "Train_y = train_data['event_type']\n",
    "\n",
    "Test_X = test_data.drop('event_type', axis=1)\n",
    "Test_y = test_data['event_type']\n",
    "\n",
    "# split the data into training and testing sets 80/20\n",
    "Train_X_train, Train_X_test, Train_y_train, Train_y_test = train_test_split(Train_X,\n",
    "                                                                            Train_y,\n",
    "                                                                            test_size=0.2,\n",
    "                                                                            random_state=12)\n",
    "\n",
    "Test_X_train, Test_X_test, Test_y_train, Test_y_test = train_test_split(Test_X,\n",
    "                                                                            Test_y,\n",
    "                                                                            test_size=0.2,\n",
    "                                                                            random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a decision tree instance and train it on the training data\n",
    "train_des_tree = DecisionTreeClassifier()\n",
    "train_des_tree.fit(Train_X_train, Train_y_train)\n",
    "\n",
    "test_des_tree = DecisionTreeClassifier()\n",
    "test_des_tree.fit(Test_X_train, Test_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the test data\n",
    "train_y_pred = train_des_tree.predict(Train_X_test)\n",
    "\n",
    "# calculate F1 score \n",
    "train_f1 = f1_score(Train_y_test, train_y_pred, average='weighted')\n",
    "print(f\"Train dataset F1 Score: {train_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a confusion matrix\n",
    "cm = confusion_matrix(Train_y_test, train_y_pred)\n",
    "\n",
    "# plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=train_des_tree.classes_,\n",
    "            yticklabels=train_des_tree.classes_)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the test data\n",
    "test_y_pred = test_des_tree.predict(Test_X_test)\n",
    "\n",
    "# calculate F1 score \n",
    "test_f1 = f1_score(Test_y_test, test_y_pred, average='weighted')\n",
    "print(f\"Test dataset F1 Score: {test_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a confusion matrix\n",
    "cm = confusion_matrix(Test_y_test, test_y_pred)\n",
    "\n",
    "# plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=test_des_tree.classes_,\n",
    "            yticklabels=test_des_tree.classes_)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "Decision Tree shows an impressive 90% F1 score result which hits the target F1 score > 80%. It takes much less time to onboard the algorithm and start using it for prediction in comparison with deep learning training. However, the score is slightly worse than RNN results which might be sighnificat for use-cases that rely on more accurate prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retraining with K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model on a different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "test_data = pd.read_csv('data/processed_data_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
